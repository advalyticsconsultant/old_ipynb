{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3101bc7-2d60-4061-addc-0fe1c345aec9",
   "metadata": {},
   "source": [
    "# Scaling values\n",
    "<div style='display: flex;'>\n",
    "    <div style='width: 3%;'></div>\n",
    "    <div>\n",
    "        <h1 style='text-align: center;'>Values before scaling</h1>\n",
    "        <img style='border: 1px black solid; margin: auto;' src='https://raw.githubusercontent.com/puneettrainer/pics/main/before-scaling.png'>\n",
    "    </div>\n",
    "    <div style='width: 3%;'></div>\n",
    "    <div>\n",
    "        <h1 style='text-align: center;'>Values after scaling</h1>\n",
    "        <img style='border: 1px black solid; margin: auto;' src='https://raw.githubusercontent.com/puneettrainer/pics/main/after-scaling.png'>\n",
    "    </div>\n",
    "    <div style='width: 3%;'></div>\n",
    "</div>\n",
    "\n",
    "Scaling is a method through which we transform numeric values in our data so that they fall in the same range and are still proportional to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04303cf1-80b4-4b5f-ac4f-9cf910b1fae2",
   "metadata": {},
   "source": [
    "### Why scale values?\n",
    "\n",
    "Scaling numeric values makes sure that features which have larger values (for example `Profit`, `Sales`) don't overshadow/take importance from features which have smaller values (for example `Discount`, `Quantity`).\n",
    "\n",
    "This can also reduce the time it takes to train a machine learning model as all the values fall on a similar scale and it can easily form patterns by giving each feature equal importance. If the values are not scaled, the machine learning algorithm will not be precisely able to align a relation and consume time to arrive at an optimal relation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d28be0f-fd79-448f-a5a6-8e4c86e59966",
   "metadata": {},
   "source": [
    "### Common Scalling Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff1cc44-a4c6-4ec9-b03a-ea6caae05555",
   "metadata": {},
   "source": [
    "#### Z-score Normalization\n",
    "\n",
    "Transforms the values so that they are normally distributed. \n",
    "\n",
    "## $value_{scaled}$ = $\\frac{x - avg(x)}{std(x)}$\n",
    "\n",
    "- since regression models are designed with the assumption of normally distributed features, `z-score` normalization can help in transforming features to be normally distributed.\n",
    "- as it uses `average` and `standard deviation`, it is still impacted by outliers, but less impacted compared to `min-max` normalization. Outliers can influence the `z-score` of a value, resulting in a slightly skewed distribution, rather than a normal distribution.\n",
    "- not suitable for feature(s) which is/are not normally distributed.\n",
    "- requires the `scaler` to be trained as per new data.\n",
    "- `z-scores` calculated for one dataset cannot be applied to another dataset.\n",
    "- does not have a fixed range of values.\n",
    "\n",
    "To use `z-score` normalization, we simply import the `StandardScaler` class from the `preprocessing` sub-module of `sklearn`. This class is used to instantiate an object of the type `StandardScaler`, which we then use to scale the values.\n",
    "\n",
    "\n",
    "Creating an object of the StandardScaler class and training it using numeric fields in the training data:\n",
    "```\n",
    "scaler = StandardScaler().fit(training_data[numeric_fields])\n",
    "```\n",
    "\n",
    "Using the scaler to scale the data\n",
    "```\n",
    "training_data[numeric_fields] = scaler.transform(training_data[numeric_fields])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d34fcdf-c136-4833-8856-276e4f1a2a6f",
   "metadata": {},
   "source": [
    "#### Normalization\n",
    "\n",
    "Transforms the values within a fixed range, 0 - 1.\n",
    "\n",
    "## $value_{scaled}$ = $\\frac{x-min(x)}{max(x)-min(x)}$\n",
    "\n",
    "- useful when the data does not contain outliers\n",
    "- it is simpler to compute.\n",
    "- since it uses `min` and `max`, this method of normalization is heavily impacted by outliers.\n",
    "\n",
    "To use `normalization`, we simply import the `MinMaxScaler` class from the `preprocessing` sub-module of `sklearn`. This class is used to instantiate an object of the type `MinMaxScaler`, which we then use to scale the values.\n",
    "\n",
    "\n",
    "Creating an object of the StandardScaler class and training it using numeric fields in the training data:\n",
    "```\n",
    "scaler = MinMaxScaler().fit(training_data[numeric_fields])\n",
    "```\n",
    "\n",
    "Using the scaler to scale the data\n",
    "```\n",
    "training_data[numeric_fields] = scaler.transform(training_data[numeric_fields])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240899b0-61e2-4c68-8520-526a47a99df9",
   "metadata": {},
   "source": [
    "#### Robust Scaling\n",
    "\n",
    "Scales the value on the basis of median and the interquartile range.\n",
    "\n",
    "## $value_{scaled}$ = $\\frac{x-median}{IQR}$\n",
    "\n",
    "- useful when data has outliers; since it scales the values on the basis of the `median` and the `interquartile range`, any effect outliers may have on the scaling are eliminated.\n",
    "- should not be used when the feature(s) is/are expected to have outliers; in some cases, outliers may be important to consider while training the model.\n",
    "- if the feature(s) is/are already normally distributed, `z-score` scaling would be more efficient.\n",
    "\n",
    "To use `robust scaling`, we simply import the `RobustScaler` class from the `preprocessing` sub-module of `sklearn`. This class is used to instantiate an object of the type `RobustScaler`, which we then use to scale the values.\n",
    "\n",
    "\n",
    "Creating an object of the StandardScaler class and training it using numeric fields in the training data:\n",
    "```\n",
    "scaler = RobustScaler().fit(training_data[numeric_fields])\n",
    "```\n",
    "\n",
    "Using the scaler to scale the data\n",
    "```\n",
    "training_data[numeric_fields] = scaler.transform(training_data[numeric_fields])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdb0e36-cacb-434a-99cd-e4656a828a3e",
   "metadata": {},
   "source": [
    "#### MaxAbs Scaling\n",
    "\n",
    "Scales the feature(s) as a ratio of the max absolute value.\n",
    "\n",
    "## $value_{scaled}$ = $\\frac{x}{max(|x|)}$\n",
    "\n",
    "- does not shift the center of the feature(s); more suited for feature(s) which have both positive and negative values - is highly spread.\n",
    "- efficient to compute.\n",
    "- since it uses `max` for computation, it is heavily impacted by outliers.\n",
    "\n",
    "To use `MaxAbs scaling`, we simply import the `MaxAbsScaler` class from the `preprocessing` sub-module of `sklearn`. This class is used to instantiate an object of the type `MaxAbsScaler`, which we then use to scale the values.\n",
    "\n",
    "Creating an object of the StandardScaler class and training it using numeric fields in the training data:\n",
    "```\n",
    "scaler = MaxAbsScaler().fit(training_data[numeric_fields])\n",
    "```\n",
    "\n",
    "Using the scaler to scale the data\n",
    "```\n",
    "training_data[numeric_fields] = scaler.transform(training_data[numeric_fields])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79305eb1-f5b7-42e9-b9da-67a8e3eb531b",
   "metadata": {},
   "source": [
    "# Encoding Values\n",
    "\n",
    "Since `logistic regression` is a linear model, it expects input features to be numeric. In order to use categorical fields, we need to convert or `encode` the categorical values.\n",
    "\n",
    "This can be done using `encoders` available in the `preprocessing` sub-module of the `sklearn` library.\n",
    "\n",
    "Some common encoders are:\n",
    "- `One Hot Encoder`: encodes the values such that each value is represented in different columns and the value of these columns is $0$ or $1$ (`True` or `False`).\n",
    "\n",
    "For example,\n",
    "\n",
    "| ID | State |\n",
    "| --- | --- |\n",
    "| 1 | Haryana |\n",
    "| 5 | Delhi |\n",
    "| 6 | Kerala |\n",
    "\n",
    "is converted to:\n",
    "\n",
    "| ID | is_Delhi | is_Haryana | is_Kerala |\n",
    "| --- | --- | --- | --- |\n",
    "| 1 | 0 | 1 | 0 |\n",
    "| 5 | 1 | 0 | 0 |\n",
    "| 6 | 0 | 0 | 1 |\n",
    "\n",
    "| Advantages | Disadvantages |\n",
    "| --- | --- |\n",
    "| retains information of the data allowing the model to identify and learn patterns at a deeper level of the data  | increases the size of the training data, making it resource intensive |\n",
    "| does not add any form of `ordinality` | loses ordinal information |\n",
    "| automatically handles missing values | may cause overfitting with simpler model |\n",
    "|  | may not handle rare values as they may not be available in the training data (or at the time of training the model), causing the model to not understand these values |\n",
    "|  | encoded columns may be correlated to each other |\n",
    "|  | not suitable for large datasets |\n",
    "\n",
    "Using `One Hot Encoder` from `sklearn.proprocessing`:\n",
    "```\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder().fit(training_data[categorical_inputs])\n",
    "training_data.loc[:, encoder.get_feature_names_out()] = encoder.transform(training_data[categorical_fields]).toarray()\n",
    "```\n",
    "#### NOTE: since `OneHotEncoder` creates a boolean PIVOT of the original columns, it outputs multiple fields. We use encoder.get_feature_names_out() to get the names of the new columns generated by the `OneHotEncoder` and add them to the training dataset.\n",
    "\n",
    "- `Label Encoding`: encodes the values by assigning a unique integer to them.\n",
    "\n",
    "| Advantages | Disadvantages |\n",
    "| --- | --- |\n",
    "| does not increase the size of the data - computationally less resource intensive | may not be suitable in some cases as the model may mistake the encoded values as continuous values |\n",
    "| retains information of `ordinality` | ordinality retained may not be true to the actual ordinality |\n",
    "| automatically handles missing values | may introduce multicollinearity; this may impact the relations identified |\n",
    "| suitable for large datasets | can only encode one field at a time |\n",
    "\n",
    "Using `Label Encoder` from `sklearn.proprocessing`:\n",
    "```\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder().fit(training_data[categorical_inputs])\n",
    "training_data.loc[:, categorical_field] = encoder.transform(training_data[categorical_field])\n",
    "```\n",
    "\n",
    "- `Mean Encoding`: replaces the original value with the mean of the target values that belong to this class.\n",
    "\n",
    "| Advantages | Disadvantages |\n",
    "| --- | --- |\n",
    "| does not increase the size of the data - computationally less resource intensive | may not be suitable in some cases as the model may mistake the encoded values as continuous values |\n",
    "| adds more value between the categorical field and the target field | patterns identified and learned by the model depend on the sample, so it may be biased to the sample and not useful in case of real-world application |\n",
    "| automatically handles missing values | may introduce multicollinearity; this may impact the relations identified |\n",
    "| suitable when there are a wide set of distinct values in the categorical field | since it uses mean, patterns identified may be exagerrated by outliers |\n",
    "| easy to interpret | |\n",
    "\n",
    "Using `Mean Encoder` from `sklearn.proprocessing`:\n",
    "```\n",
    "from sklearn.preprocessing import TargetEncoder\n",
    "\n",
    "encoder = TargetEncoder().fit(training_data[categorical_fields], training_data[target_field])\n",
    "training_data.loc[:, categorical_fields] = encoder.transform(training_data[categorical_fields])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6882570-ffbe-4b0b-be74-772d76f9565d",
   "metadata": {},
   "source": [
    "### Creating a model to predict whether a customer will churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ae95a9-aa6f-478e-97e7-c90956cb31d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75966122-fb5c-4747-8ffb-9c0bd47b19d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "churn = pd.read_csv(r\"https://raw.githubusercontent.com/puneettrainer/datasets/main/customer_churn.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1e09fb-b8a9-4da3-a435-f87c9d146dd7",
   "metadata": {},
   "source": [
    "### Segregating `target` and `input` fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db61d990-7d76-4b22-9bb2-d82dda8daf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_field = 'churn'\n",
    "fields = list(churn.columns)\n",
    "fields.remove(target_field)\n",
    "input_fields = fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e632450c-73e6-45a3-98ef-d17b4a28370b",
   "metadata": {},
   "source": [
    "### Segregating `numeric` and `non-numeric` input fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daaf5c0-fe0d-4adf-9091-3716edce76d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_fields = list(churn[input_fields].select_dtypes(exclude='number').columns)\n",
    "numeric_fields = list(churn[input_fields].select_dtypes(include='number').columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f4670b-65fa-4ac3-93f6-b095919e88b1",
   "metadata": {},
   "source": [
    "### Splitting dataset into `training` and `test` data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee38e12-ba67-4491-87c5-7be7e3408144",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, test_data = train_test_split(churn\n",
    "                                           ,test_size=0.3\n",
    "                                           ,random_state=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1959b569-bc9d-422f-8ec8-e5bd50f60035",
   "metadata": {},
   "source": [
    "### Scaling numeric fields using `RobustScaler`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effd2dac-e3d2-4ecb-8e5b-364fac6d4424",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler().fit(training_data[numeric_fields])\n",
    "training_data.loc[:, numeric_fields] = scaler.transform(training_data[numeric_fields])\n",
    "test_data.loc[:, numeric_fields] = scaler.transform(test_data[numeric_fields])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9509aa-b7a7-46de-81fa-703ea0bc77cc",
   "metadata": {},
   "source": [
    "### Encoding categorical fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d6dafb-2d92-420c-ac4d-943e02ed8ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder().fit(training_data[categorical_fields])\n",
    "training_data.loc[:, encoder.get_feature_names_out()] = encoder.transform(training_data[categorical_fields]).toarray()\n",
    "test_data.loc[:,  encoder.get_feature_names_out()] = encoder.transform(test_data[categorical_fields]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f543b720-06c9-4013-a9cc-4b16ac5b42f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modifying list of input fields after encoding\n",
    "input_fields = list(encoder.get_feature_names_out()) + numeric_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4016485d-51f2-4a4b-9ddd-4a65fd1de93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression().fit(training_data[input_fields], training_data[target_field])\n",
    "predictions = model.predict(test_data[input_fields])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a8c76d-1a81-44d0-965d-2da54a2ce0d4",
   "metadata": {},
   "source": [
    "### Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e2bbbe-9969-4516-8420-b208df730f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Accuracy score: {accuracy_score(test_data[target_field], predictions)}')\n",
    "print(f'Precison score: {precision_score(test_data[target_field], predictions)}')\n",
    "print(f'Sensitivity score: {recall_score(test_data[target_field], predictions)}')\n",
    "print(f'F1 score: {f1_score(test_data[target_field], predictions)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09043db-eefb-415e-a03d-cbfa2350d027",
   "metadata": {},
   "source": [
    "As per the above evaluation metrics:\n",
    "- `Accuracy score` is $82.03\\% \\implies$  the model makes correct predictions $82.03\\%$ of the time. This means that it correctly identifies `True Positive`s and `True Negatives` $82.03\\%$ of the time. Fair enough for an un-optimized model.\n",
    "- `Precision score` is $60.08\\% \\implies$ out of the actual customers who churned, the model was correctly able to predict $60.08\\%$ customers. This level of correctness is not good enough to make a final decision on whether a new customer would churn or not: this model cannot be used for identifying customers who will churn or not. If the precision score was higher, we could use this model to predict whether to accept a new customer or not.\n",
    "- `Sensitivity score` is $22.94\\% \\implies$ out of the customers the model predicted to churn, it only predicted $22.94\\%$ of them to churn. This level of correctness is also not good enough to identify customers who potentially may churn. If the sensitivity score was higher, we could use this model to identify customers who may potentially churn and try to strategize on how to prevent them from churning.\n",
    "- `F1 Score` is $33.20\\% \\implies$ the comprehensive score of the model consider `precision` and `recall`. This metric is evaluating how likely the model would classify a customer to churn. If the `F1 score` was higher, we could use this model in our decision making process. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c2a2f1-4c7d-45c6-88c7-988b90c3f366",
   "metadata": {},
   "source": [
    "### Logistic Regression Coefficients\n",
    "\n",
    "Just like the `LinearRegression` class provides the `.coef_` attribute, we can use the `.coef_` attribute to view the weights the model has assigned to different input features and decide which input fields would be relevant to keep, and which fields need to be used as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6aae367-32aa-4b21-bc31-9753b3e39cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b7396e-2e52-4903-877b-7c9d477e40e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing weights in a dataframe\n",
    "weights = pd.DataFrame({'feature': model.feature_names_in_.reshape(-1)\n",
    "                       ,'weight': model.coef_.reshape(-1)})\n",
    "\n",
    "# sorting features by their weight\n",
    "weights.sort_values(by=['weight'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa77a4a-595f-4f7e-a342-826ac7239112",
   "metadata": {},
   "source": [
    "Based on the above weights, we can conclude that the model is assigning more importance to `age`, `balance`, `country`, `gender`, `active_member`: the remaining fields don't significantly impact the model performance. So we can remove these fields from the input features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f7ab22-c16c-4b83-bb0b-82376a706a03",
   "metadata": {},
   "source": [
    "### Saving the model for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efce3c1-3107-4429-86b5-5a314b073afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_field = 'churn'\n",
    "input_fields = ['age', 'balance', 'country', 'gender', 'active_member']\n",
    "\n",
    "categorical_fields = list(churn[input_fields].select_dtypes(exclude='number').columns)\n",
    "numeric_fields = list(churn[input_fields].select_dtypes(include='number').columns)\n",
    "\n",
    "training_data, test_data = train_test_split(churn\n",
    "                                           ,test_size=0.3\n",
    "                                           ,random_state=4)\n",
    "\n",
    "scaler = RobustScaler().fit(training_data[numeric_fields])\n",
    "training_data.loc[:, numeric_fields] = scaler.transform(training_data[numeric_fields])\n",
    "test_data.loc[:, numeric_fields] = scaler.transform(test_data[numeric_fields])\n",
    "\n",
    "encoder = OneHotEncoder().fit(training_data[categorical_fields])\n",
    "training_data.loc[:, encoder.get_feature_names_out()] = encoder.transform(training_data[categorical_fields]).toarray()\n",
    "test_data.loc[:,  encoder.get_feature_names_out()] = encoder.transform(test_data[categorical_fields]).toarray()\n",
    "\n",
    "# modifying list of input fields after encoding\n",
    "model_input_fields = list(encoder.get_feature_names_out()) + numeric_fields\n",
    "\n",
    "model = LogisticRegression().fit(training_data[model_input_fields], training_data[target_field])\n",
    "predictions = model.predict(test_data[model_input_fields])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca63b83-8669-4ce6-9f4a-136f3cd5f111",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib as jb\n",
    "\n",
    "churn_model = {'target_field': target_field\n",
    "              ,'input_fields': input_fields\n",
    "              ,'categorical_fields': categorical_fields\n",
    "              ,'numeric_fields': numeric_fields\n",
    "              ,'model_inputs': model_input_fields\n",
    "              ,'scaler': scaler\n",
    "              ,'encoder': encoder\n",
    "              ,'model': model}\n",
    "\n",
    "jb.dump(churn_model, 'churn_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017a9a3f-8fb0-4ea6-bd71-b3283aad8d26",
   "metadata": {},
   "source": [
    "### Reusing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5ee5bb-ef22-4fc7-87a5-2755741b42b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib as jb\n",
    "import pandas as pd\n",
    "\n",
    "saved_model = jb.load('churn_model.joblib')\n",
    "\n",
    "# creating an empty dictionary to store input values\n",
    "input_values = {}\n",
    "\n",
    "for feature in saved_model['input_fields']:\n",
    "    # fetching input values from user\n",
    "    value = input(f'Enter the {feature.lower()}: ')\n",
    "\n",
    "    # inserting input values into input_values dictionary\n",
    "    input_values.update({feature:value})\n",
    "\n",
    "# converting numeric inputs to numbers\n",
    "for field in numeric_fields:\n",
    "    input_values[field] = float(input_values[field])\n",
    "\n",
    "# converting data into a dataframe\n",
    "input_record = pd.DataFrame(input_values, index=[0])\n",
    "\n",
    "# scaling numeric values\n",
    "input_record[numeric_fields] = saved_model['scaler'].transform(input_record[numeric_fields])\n",
    "\n",
    "# encoding categorical values\n",
    "input_record.loc[:, saved_model['encoder'].get_feature_names_out()] = saved_model['encoder'].transform(input_record[saved_model['categorical_fields']]).toarray()\n",
    "\n",
    "# making prediction more readable\n",
    "if saved_model['model'].predict(input_record[saved_model['model_inputs']]) == [1]:\n",
    "    prediction = 'Churn'\n",
    "else:\n",
    "    prediction = \"Won't Churn\"\n",
    "\n",
    "# displaying predicted value\n",
    "print(f'Predicted status for provided values is: {prediction}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4369fa8-5548-4d8e-8a10-e6cc8713560f",
   "metadata": {},
   "source": [
    "### Creating a model to predict whether customer is satisfied or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e96cfe-da99-438e-b2f7-1b7026098fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing data\n",
    "airline = pd.read_csv(r\"E:\\data\\airline_satisfaction.csv\")\n",
    "\n",
    "# saving target and input field names\n",
    "target_field = 'satisfaction'\n",
    "\n",
    "input_fields = list(airline.columns)\n",
    "input_fields.remove(target_field)\n",
    "\n",
    "# segregating categorical and numeric fields\n",
    "categorical_fields = list(airline[input_fields].select_dtypes(exclude='number').columns)\n",
    "numeric_fields = list(airline[input_fields].select_dtypes(include='number').columns)\n",
    "\n",
    "# splitting dataset into training and testing data\n",
    "training_data, test_data = train_test_split(airline\n",
    "                                           ,test_size=0.3\n",
    "                                           ,random_state=4)\n",
    "\n",
    "# scaling numeric fields in training and test data\n",
    "scaler = RobustScaler().fit(training_data[numeric_fields])\n",
    "training_data.loc[:, numeric_fields] = scaler.transform(training_data[numeric_fields])\n",
    "test_data.loc[:, numeric_fields] = scaler.transform(test_data[numeric_fields])\n",
    "\n",
    "# encoding categorical fields in training and test data\n",
    "encoder = OneHotEncoder().fit(training_data[categorical_fields])\n",
    "training_data.loc[:, encoder.get_feature_names_out()] = encoder.transform(training_data[categorical_fields]).toarray()\n",
    "test_data.loc[:,  encoder.get_feature_names_out()] = encoder.transform(test_data[categorical_fields]).toarray()\n",
    "\n",
    "# modifying list of input fields after encoding\n",
    "input_fields = list(encoder.get_feature_names_out()) + numeric_fields\n",
    "\n",
    "# instantiating model and computing predictions\n",
    "model = LogisticRegression().fit(training_data[input_fields], training_data[target_field])\n",
    "predictions = model.predict(test_data[input_fields])\n",
    "\n",
    "# displaying evaluation metrics\n",
    "print(f'Accuracy score: {accuracy_score(test_data[target_field], predictions)}')\n",
    "print(f'Precison score: {precision_score(test_data[target_field], predictions, pos_label='satisfied')}')\n",
    "print(f'Sensitivity score: {recall_score(test_data[target_field], predictions, pos_label='satisfied')}')\n",
    "print(f'F1 score: {f1_score(test_data[target_field], predictions, pos_label='satisfied')}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
