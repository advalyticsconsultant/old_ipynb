{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48a0f9b0-36c8-469e-8733-9a88be6554ac",
   "metadata": {},
   "source": [
    "### Gaussian Distribution\n",
    "\n",
    "Gaussian (or normal) distribution is a statistical distribution in which all values are centrally spread. This means that most of the values in the data are around the average value and the standard deviation is $0$ or close to $0$. Normal distribution is plotted for the count of observations, rather than the raw values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ff1055-900e-46fe-9d57-128e3f01651d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4ab863-5918-4626-9533-0ebc4723ebf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loans = pd.read_csv(r\"https://raw.githubusercontent.com/puneettrainer/datasets/main/bankloan.csv\")\n",
    "loans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4561d87-dc3f-4805-bd81-b8f728ac207a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating distribution of Experience\n",
    "plt.bar(loans['Experience'].value_counts().index, loans['Experience'].value_counts())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a473cc-d259-4c4c-af24-803a6b8732fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "loans['Experience'].value_counts().std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d0113a-38fd-4855-8ea1-f28f26238e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating distribution of Income\n",
    "plt.bar(loans['Income'].value_counts().index, loans['Income'].value_counts())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940cb4ce-d470-4b20-819f-1977fa981ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loans['Income'].value_counts().std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4434faea-e19a-4922-ab98-c2403dc558ec",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "In `linear regression`, we attempt to predict the value of the `target` variable based on the values of the input variable by fitting a line such that all the data points are as close to this line as possible. By simply intersecting this line from the `x-axis`, we can trace back to the `y-axis` to get our predicted value.\n",
    "\n",
    "<img style='margin: auto; width: 400px;' src='https://raw.githubusercontent.com/puneettrainer/pics/main/linreg.png'>\n",
    "<h4 style='text-align: center;'>Linear Regression - $y = m * x + c$</h4>\n",
    "\n",
    "In case of `logistic regression`, we attempt to predict the `probability` of something being true or not; our prediction can be either `True` or `False`. This makes the `y-axis` for logistic regression have the range 0 to 1. This also makes it computationally difficult to fit a line such that it is as close to all the data points as possible. To overcome this, the relation is created (line is fit) by creating a line which has the maximum likelihood.\n",
    "\n",
    "<img style='margin: auto; width: 450px;' src='https://raw.githubusercontent.com/puneettrainer/pics/main/logreg-prob.png'>\n",
    "<h4 style='text-align: center;'>Logistic Regression using Maximum Likelihood</h4>\n",
    "\n",
    "In terms of finding the best fitting line, logistic regression is similar to linear regression, but with one small change. We start off by converting the `y-axis` from `probability` to the $ln$ (`natural log`) of probability.<br>\n",
    "$probability =$ $\\frac{p}{1-p}$\n",
    "\n",
    "$ln(probability) =$ $ln(\\frac{p}{1-p})$\n",
    "\n",
    "This function is called the `logit function`. When we transform probability using $ln$, we get a new `y-axis` with the range as follows:\n",
    "\n",
    "Upper range in old `y-axis` = 1<br>\n",
    "Upper range in new `y-axis`<br>\n",
    "&emsp; = $ln(\\frac{1}{1 - 1})$<br>\n",
    "&emsp; = $ln(\\frac{1}{0})$<br>\n",
    "&emsp; = $ln(1) - ln(0)$<br>\n",
    "&emsp; = 0 - ($- \\infty$)<br>\n",
    "&emsp; = $+\\infty$\n",
    "\n",
    "Upper range in old `y-axis` = 0<br>\n",
    "Upper range in new `y-axis`<br>\n",
    "&emsp; = $ln(\\frac{0}{1 - 0})$<br>\n",
    "&emsp; = $ln({0})$<br>\n",
    "&emsp; = $- \\infty$<br>\n",
    "\n",
    "In this new graph, we create a line which gives the maximum likelihood to the line in the original graph.\n",
    "\n",
    "<img style='margin: auto; width: 450px;' src='https://raw.githubusercontent.com/puneettrainer/pics/main/logreg-log.png'>\n",
    "\n",
    "In this graph, we are viewing the log of probability. Since this can extend to $\\infty$, calculating the distance of points (to figure out which line is as close to all points as possible) is computationally difficult. Instead we trace the point on this line where it intersects with the values in the `x-axis`. These values (on this line) are in the form of log of probabilities. To plot them on the original graph (with probability on the `y-axis`), we convert them back to probabilities.<br>\n",
    "$ln(probability) =$ $ln(\\frac{p}{1-p})$\n",
    "\n",
    "From the line in the new chart, we have values of $ln(probability)$. We want to find the value of `p`.<br>\n",
    "$ln(probability) =$ $ln(\\frac{p}{1-p})$<br>\n",
    "Exponentiating the log of probabilities, we get:<br>\n",
    "$e^{ln(probability)} = \\frac{p}{1-p}$<br>\n",
    "$\\implies p = e^{ln(probability)} \\times (1-p)$<br>\n",
    "$\\implies p = e^{ln(probability)} - (e^{ln(probability)} \\times p)$<br>\n",
    "$\\implies p + (e^{ln(probability)} \\times p) = e^{ln(probability)}$<br>\n",
    "$\\implies p + (1 + e^{ln(probability)}) = e^{ln(probability)}$<br>\n",
    "$\\implies p = \\frac{e^{ln(probability)}}{(1 + e^{ln(probability)})}$<br>\n",
    "\n",
    "This function is called the `sigmoid` function. By plugging in the values of $e^{ln(probability)}$, we can now get the predicted probabilities and plot them on the original chart (with probability as `y-axis`). This value is called `likelihood`. The algorithm calculates the product of these values and selects the model which has the highest product. Since the way `logistic regression` predicts values is still built on trying to fit a line (even if here we are trying to get the maximum likelihood), it is categorised as a `regression` model and not a `classification` model. Also, since `logistic regression` predicts the probability of something being `True` (or `False`) it is used applicable when we want to perform `binary classification`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4369fa8-5548-4d8e-8a10-e6cc8713560f",
   "metadata": {},
   "source": [
    "### Creating a model to predict whether loan would be approved or not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d58045-e052-4d5e-84d6-b94ed2f4863a",
   "metadata": {},
   "source": [
    "### Choosing columns as `input features` of the model\n",
    "\n",
    "Before choosing fields as `input features` of the model, we figure out the correlation of fields to the `target`.\n",
    "\n",
    "We also avoid choosing fields which are highly correlated to each other. This causes `linear models` (linear regression, logistic regression) to not understand the individual effect of a feature on the target and can result in models which are inaccurate.\n",
    "\n",
    "Input features should also be `diverse`. By having a wider set of values, the model can learn more patterns. For example, if we have more records where applicants have 10 family members compared to records where applicants don't have as many members, the model may learn that applicants generally have 10 family members. For this reason, input features chosen should ideally have a `normal distribution`.\n",
    "\n",
    "Input features are also chosen as per their relevance, assessed by domain knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26a899f-1d97-4a76-bfa7-0e3d9ddbc906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation of all fields with the target field\n",
    "loans.corr().loc[:, 'Personal.Loan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5423aed9-6d63-4626-86a3-e4c94f40e8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation of fields (\"highly\" correlated with the target), amongst themselves\n",
    "# highly here simply means correlation is greater than 0.1\n",
    "selected_features = ['Income', 'CCAvg', 'Education', 'Mortgage', 'CD.Account']\n",
    "loans.corr().loc[selected_features, selected_features + ['Personal.Loan']].sort_values(by=['Personal.Loan'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9aa648a-36b9-40b5-ab02-7a5ba15231d8",
   "metadata": {},
   "source": [
    "From the above, `CCAvg` and `Income` are strongly correlated, so we choose either one of them. Seeing that `Income` is more correlated to the target than `CCAvg`, we will choose `Income`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878d8c71-13e3-4ce9-b45e-385307e80685",
   "metadata": {},
   "outputs": [],
   "source": [
    "loans.describe().loc['std', ['Income', 'CD.Account', 'Personal.Loan']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00284d33-9270-44ba-acd1-fbb8c008990b",
   "metadata": {},
   "source": [
    "Since there is no significantly large `standard deviation` value, we can continue with these fields as `input features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef911f5-52fe-4232-a31e-c4ddff5534ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_fields = ['Income', 'CD.Account', 'Personal.Loan']\n",
    "target_field = 'Personal.Loan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8515c585-096a-490c-a4cd-33f91f3ef772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the dataset into training and testing datasets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "training_data, test_data = train_test_split(loans\n",
    "                                           ,test_size=0.3\n",
    "                                           ,random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d60ec86-f323-4aef-bf24-f0d9343feb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# instantiating the model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# training the model\n",
    "model.fit(training_data[input_fields], training_data[target_field])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1382b1-50e7-44e2-8e63-a93f61011a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_data[input_fields])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e1b08c-617a-4c51-a251-9c9f29457f30",
   "metadata": {},
   "source": [
    "### Evaluating a Logistic Regression model\n",
    "\n",
    "Since the output of a logistic regression model is not a continuous value, we cannot use `MAE`, `MSE` or `RMSE` to evaluate the performance of our model.\n",
    "\n",
    "| Result | Actual Status | Model Status |\n",
    "| --- | --- | --- |\n",
    "| True Postive | Approved | Approved |\n",
    "| False Positive | Not Approved | Approved |\n",
    "| True Negative | Not Approved | Not Approved |\n",
    "| False Negative | Approved | Not Approved |\n",
    "\n",
    "Some commonly used evaluation metrics for logistic regression are:\n",
    "- `Accuracy`: simply the ratio of correct predictions to the total number of predictions.\n",
    "\n",
    "$Accuracy = \\frac{True \\: Positive \\: + \\: True \\: Negative}{True \\: Positive \\: + \\: False \\: Positive \\: + \\: True \\: Negative \\: + \\: False \\: Negative} = \\frac{correct\\:predictions}{total\\:predictions}$\n",
    "\n",
    "This is a straightforward metric which indicates the ratio how many \"correct\" predictions are made by the model. It is not very indicative of the correctness of the model when the model is `imbalanced`; there are more observations of one type of class than another.\n",
    "\n",
    "- `Precision`: ratio of `True Positive` to the total number of `True` predictions.\n",
    "\n",
    "$Precision = \\frac{True\\:Positive}{True\\:Positive\\:+\\:False\\:Positive}$\n",
    "\n",
    "Since this metric focuses only on positive outcomes, it does not evaluate the model's performance in case of negative outcomes. Due to this reason, it is also not suitable for models where the dataset is imbalanced with more negative outcomes. Models with a high `Precision` score may evaluate, for example, an applicant as being a defaulter (even if they are not).\n",
    "\n",
    "- `Sensitivity`: ratio of `True Positive` to the sum of `True Positive` and `False Negative`.\n",
    "\n",
    "$Sensitivity = \\frac{True\\:Positive}{True\\:Positive\\:+\\:False\\:Negative}$\n",
    "\n",
    "This metric is suitable in case of imbalanced datasets where there are more `False` instances than `True`. However, as it only evaluates `True Positive`s, it may not correctly indicate the portion of predictions which were `False Positives`.\n",
    "\n",
    "- `F1 Score`: this evaluation metric is used in case of imbalanced datasets and takes into account the `False Negative`s. Due to this, it is a comprehensive way of evaluating a model. However, it gives equal weightage to both `Precision` and `Sensitivity`; this means that even if the model  may have a precision score, the F1 score may be low due to low value of the sensitivity score (or vice-versa).<br>\n",
    "\n",
    "$F1\\:Score = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb882784-9353-4264-8db0-3b2690f94638",
   "metadata": {},
   "source": [
    "### Creating a `Confusion Matrix`\n",
    "\n",
    "`Confusion Matrix` is a convenient way to view all the possible outcomes of a classification model. It clearly computes the `True Positive`s, `False Positive`s, `True Positive`s and `False Positive`s of the model.\n",
    "\n",
    "In the `sklearn` library, it can be accessed in the `metrics` sub-module.\n",
    "\n",
    "### `confusion_matrix(actual, prediction)`\n",
    "\n",
    "Structure of the output:\n",
    "\n",
    "|  | Predicted True | Predicted False |\n",
    "| --- | --- | --- |\n",
    "| <b>Actual True</b> |  |  |\n",
    "| <b>Actual False</b> |  |  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f0bc18-c4df-485a-8938-c2608b766427",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(test_data[target_field], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c2003f-35c6-4377-8505-9f0d519a97b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(test_data[target_field], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43659b1d-bc15-4e03-a503-0d3a2e3c5b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "precision_score(test_data[target_field], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c05db47-c743-449e-990c-eb5a64d55984",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "recall_score(test_data[target_field], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7d9c1e-1d4a-4477-9041-fb4fdeaebd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(test_data[target_field], predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8120eafa-3632-4017-86e3-229bd8ff1ccd",
   "metadata": {},
   "source": [
    "### Saving model for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6150b9-a122-458f-80e5-50c13e3fc81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib as jb\n",
    "\n",
    "# save model\n",
    "loan_approval = {'inputs':input_fields\n",
    "                ,'target':target_field\n",
    "                ,'model': model}\n",
    "jb.dump(loan_approval, 'loan_approval.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
