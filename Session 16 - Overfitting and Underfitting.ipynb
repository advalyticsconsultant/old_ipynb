{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "847cbbe9-03f4-46f5-bdc6-0e1ed0083019",
   "metadata": {},
   "source": [
    "### Overfitting and Underfitting\n",
    "\n",
    "Overfitting is the phenomenon in which the machine learning model `learns the exact training data, rather than identifying and learning patterns in it`. This is problematic because training data is limited and simply modeling it does not work well for predictive purposes. After having trained the model, if it is `overfitted`, it would perform well only with the data that it has seen before, but not with data that it has not seen. The whole concept behind machine learning is to develop models which can `approximate` (predict) a particular value, based on past patterns. With overfitted models, they can only make predictions based on the data they have seen before and not new data values.\n",
    "\n",
    "The term `overfitting` comes from the fact that machine learning algorithms attempt to `fit` an optimal (mathematical) relation between the input features and the target value. If the model is `overfitted`, it simply implies that the mathematical relation that it computed for the dataset is taking into account very specific patterns that occur in the training dataset (which may not appear in future data). Another problem with `learning` the training data too well (as opposed to identifying and learning patterns in the training data) is the fact that the training dataset is simply a sample of the possible data that the model will process. There can be observations in the dataset that don't appear in the training data, which may appear in the test data (or future observations). Hence, overfitting is counterproductive to our objective.\n",
    "\n",
    "We can identify whether a model is overfitted if the evaluation score on the training data is much higher than the evaluation score on the test data.\n",
    "\n",
    "On the other hand, `underfitting` simply means that the algorithm is not able to formulate an optimal relation between the input features and target values; it has not identified and learned any patterns to base predictions on. An underfitted model performs poorly on both training data and test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883d5bcc-9f55-4bc3-a92e-cbad4e7d00ee",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "Regularization is a technique through which we identify whether a model is overfitted and control it. It identifies overfitting by adding a \"`penalty`\" to the final score of the model. This \"penalty\" is simply the aggregation of weights of the model. `For regularization to work, we need to first scale input features appropriately. It does not work with raw (unscaled) input features.`\n",
    "\n",
    "There are mainly two types of regularization methods:\n",
    "1. `Lasso` (`L1`): this adds the sum of absolute weights to the final score. It is applicable in cases where we have limited data, and a lot of input features. This technique aims to reduce the number of input features. While the model is training, the weights are `shrunk` to zero as much as possible, so that ultimately only a few input features are left out of the ones chosen.\n",
    "\n",
    "Mathematically,<br>\n",
    "$\\text{Lasso Regularization} = Score + \\alpha * \\sum |w_i|$\n",
    "\n",
    "| Advantage | Disadvantage |\n",
    "| --- | --- |\n",
    "| applicable when there are a lot of input features | not suitable when input features are correlated to each other |\n",
    "| helps selecting relavant input features | computationally more expensive |\n",
    "\n",
    "2. `Ridge` (`L2`): this adds the sum of squared weights to the final score. It is applicable in cases where the input features are correlated to each other. It aims to spread the influence of a single featire across multiple features. While the model is training, the weights assigned are small and within a small range of values.\n",
    "\n",
    "Mathematically,<br>\n",
    "$\\text{Ridge Regularization} = Score + \\alpha * \\sum (w_i ^ 2)$\n",
    "\n",
    "| Advantage | Disadvantage |\n",
    "| --- | --- |\n",
    "| applicable when input features are correlated to each other | does not help in reducing the number of input features; not very useful with datasets with a lot of input features |\n",
    "| computationally more efficient than Lasso regularization |  |\n",
    "\n",
    "There are cases when we have limited data, with a high number of input features, where the features may be correlated to each other. In such cases, we use a combination of L1 and L2 regularization, called `Elastic-Net` regularization. This method of regularization allows us to select features, and also spread the influence of a single feature across multiple features.\n",
    "\n",
    "Mathematically,<br>\n",
    "$\\text{Elastic-Net Regularization} = Score + \\alpha_L * \\sum |w_i| + \\alpha_R * \\sum (w_i ^ 2)$\n",
    "\n",
    "In all of the above, $\\alpha$ is the regularization parameter, chosen by us.\n",
    "\n",
    "| Advantage | Disadvantage |\n",
    "| --- | --- |\n",
    "| applicable when there are a lot of input features | computationally more expensive |\n",
    "| applicable to data where input features are correlated |  |\n",
    "| helps selecting relevant input features |  |\n",
    "| assigns small but similar weights to all input features so that one feature does not influence the entire model |  |\n",
    "\n",
    "Ideally, we would want the $\\text{Score}$ to be as low as possible, and if we minimize the value of the penalty the score would be near zero. If the weights have a large magnitude, it would directly result in a bad overall score for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4586124c-700f-4a7b-9406-a4501bd83225",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11412cbb-459d-47c0-b9fc-d02d8034b4eb",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "\n",
    "Linear regression is a very simple linear machine learning algorithm which allows us to predict continuous values. It forms a linear relation between the input features and the target value, which can be expressed as:<br>\n",
    "$\\text{Prediction} = w_1 * feature_1 + w_2 * feature_2 + ... + w_n * feature_n$\n",
    "\n",
    "We can easily identify which features have the most influence on the target value, which can further help in analysis. It is also computationally economical.\n",
    "\n",
    "To effectively use the algorithm, values should be scaled appropriately, in order to prevent the algorithm from giving more significance to features which have large values in terms of magnitude.\n",
    "\n",
    "Linear regression is, however, not suitable for classification purposes. It does not work well with input features which are correlated to each other (if input features are correlated, the algorithm may pick up redundant patterns, leading to poor performance).\n",
    "\n",
    "### Regularization in Linear Regression\n",
    "\n",
    "Sci-kit provides other classes to create linear regression models, which are regularized. These are:\n",
    "- Lasso\n",
    "- Ridge\n",
    "- ElasticNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17bfbed-17ba-4d11-95ea-993f3138ea55",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "Logistic regression is a linear machine learning model, which is best suited for binary classification. It can answer problem statements such as whether an observation belongs to a particular class or not, by calculating the probability of `belonging`-ness. This cut-off or `threshold` of belonging-ness is 0.5 by default, but can be customized as follows:\n",
    "- fetching probability of belonging to the positive class: the `LogisticRegression().predict_proba(training_data[input_fields])[:, 1]` method, fetches the probability of the observation belonging to the positive class.\n",
    "- comparing the probabilities above with a specific value:\n",
    "`(LogisticRegression().predict_proba(training_data[input_fields])[:, 1] >= custom_threshold).astype(int)`\n",
    "\n",
    "Since logistic regression is a linear machine learning model, it can benefit greatly by scaled numeric values.\n",
    "\n",
    "We can implement regularization in LogisticRegression by specifying a value for the `penalty` hyperparameter. This can have the following values:\n",
    "- `l1`: for lasso regularization\n",
    "- `l2`: for ridge regularization\n",
    "- `elasticnet`: for elastic-net regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941de02e-7510-43f4-b1f0-bd73de708f46",
   "metadata": {},
   "source": [
    "### Decision Trees\n",
    "\n",
    "Decision tree is a machine learning model which splits the dataset into various `nodes`. The splitting is done in such a manner that the terminating nodes of the tree comprise of samples which mostly belong to the same class or similar value range. This algorithm is very easy-to-understand and creates a logical, interpretable path from the top of the data all the way to the final classification, allowing us to better understand the influence of various features on the target value.\n",
    "\n",
    "It is computationally very expensive, hence it requires a lot of hyperparameter tuning. A standard decision tree model would be extremely overfitted over the selected input features. We can prevent overfitting by specifying customizations such as `max_depth`, `min_sample_split`, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652249db-fb44-445e-990e-9874ea1ec770",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "Random Forest is a very strong machine learning algorithm which comprises of multiple decision trees. Each tree is trained on bootstrapped data and has a different structure, allowing the overall model to capture and utilize a wide range of interactions between the various input features. It is computationally very expensive, hence its not the `go-to` algorithm for machine learning models.\n",
    "\n",
    "It can be optimized by much of the same hyperparameters used in decision trees as it is based on the decision tree algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafc57b2-d635-4b79-9424-c6571f4b299b",
   "metadata": {},
   "source": [
    "### Extreme Gradient Boost\n",
    "\n",
    "Extreme gradient boost is another very comprehensove machine learning algorithm, based on decision trees. It is also computationally very expensive and requires a lot of hyperparameter tuning. It automatically introduces generalization into the model by the use of `learning rate` ($\\eta$) in the computation of the final prediction. Learning rate is the weightage assigned to the output of each tree, which controls precisely how fitted the model is. The value for learning rate ranges between 0 and 1, since it assigns a weight to the predicted values of subsequent decision trees - a smaller learning rate means that the predictions made by the subsequent trees is scaled down, while a bigger value of learning rate implies that the predicted values are scaled up. \n",
    "\n",
    "\n",
    "We can additionally configure a regularization parameter for each leaf of the tree, which helps in further generalization of the model by reducing the `sensitivity of the predicted value`, computing a value farther from the actual value. This way, the model can make predictions on unseen data as $\\lambda$ prevents the model from learning the training data and encourages it to learn patterns of the data instead.\n",
    "\n",
    "Further hyperparameter tuning in `XGBoost`\n",
    "- `reg_lambda`: this allows us to specify the value of the regularization parameter.\n",
    "- `learning_rate`: this allows us to specify the value of the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c048888-f2b7-42bf-9d9f-021d0973fec9",
   "metadata": {},
   "source": [
    "### Principal Component Analysis\n",
    "\n",
    "Principal Component Analysis is an unsupervised machine learning algorithm which is used for dimensionality reduction. Dimensionality reduction allows us to transform input variables based on their significance. We cannot directly use PCA for making predictions, but it can allow us to optimize our data efficiently for use in any machine learning algorithm.\n",
    "\n",
    "It aims to maximize the variation of input features on the target value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de6d68c-7cf2-449b-8550-0eb5c9dfadcb",
   "metadata": {},
   "source": [
    "### Linear Discriminant Analysis\n",
    "\n",
    "Linear Discriminant Analysis is a supervised machine learning algorithm, primarily used for dimensionality reduction. It can, however, also be used for classification problems. It aims to increase the variation between the input features, while at the same time promoting separation between various classes in the data. In contrast to PCA, which is only concerned with maximizing variation between input features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3737e8-62e7-43e3-89c5-fbb98f6039bd",
   "metadata": {},
   "source": [
    "### k-Means Clustering\n",
    "\n",
    "k-Means Clustering is an unsupervised machine learning algorithm which is used to cluster observations, based on their similarities. It does not directly allow us to make predictions, but can help us understand the various types of `groups` in our dataset and help us analyze them in more specific ways to extract more insight. It cannot be used for regression problems or classification problems, but can help us extract group-specific characteristics in our data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61558f85-fd3c-4fff-bea8-60c8e38a24da",
   "metadata": {},
   "source": [
    "### Cross Validation - Finding the most optimal hyperparameters for a model\n",
    "\n",
    "Cross validation is a technique through which we can fit a machine learning model on multiple combinations of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d4244b76-1a29-4bfd-976e-a4d8619c67ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8b312f90-bda5-405d-864d-71fc2fddf783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing data\n",
    "housing_data = pd.read_csv(r'https://raw.githubusercontent.com/puneettrainer/datasets/main/housing.csv')\n",
    "\n",
    "# segregating target and input features\n",
    "target_field = 'price'\n",
    "input_fields = list(housing_data.columns)\n",
    "input_fields.remove(target_field)\n",
    "\n",
    "# segragating categorical and numeric fields\n",
    "categorical_fields = list(housing_data[input_fields].select_dtypes(include='object').columns)\n",
    "numeric_fields = list(housing_data[input_fields].select_dtypes(exclude='object').columns)\n",
    "\n",
    "# splitting the dataset into training and test data\n",
    "training_data, test_data = train_test_split(housing_data\n",
    "                                           ,test_size=0.35\n",
    "                                           ,random_state=45)\n",
    "\n",
    "# encoding categorical fields\n",
    "encoder = OneHotEncoder().fit(training_data[categorical_fields])\n",
    "training_data.loc[:, encoder.get_feature_names_out()] = encoder.transform(training_data[categorical_fields]).toarray()\n",
    "test_data.loc[:, encoder.get_feature_names_out()] = encoder.transform(test_data[categorical_fields]).toarray()\n",
    "\n",
    "input_features = list(encoder.get_feature_names_out()) + numeric_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2746c101-610f-4054-81ba-b3e7345e98a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'learning_rate': 0.04, 'max_depth': 6, 'n_estimators': 125, 'reg_lambda': 12.5}\n",
      "Best cross-validation score:  1158370.1364893843\n",
      "Test set accuracy with best model:  0.5987241268157959\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'n_estimators':[75, 125, 150]\n",
    "             ,'learning_rate': [0.045, 0.037, 0.04]\n",
    "             ,'reg_lambda': [10, 12.5]\n",
    "             ,'max_depth':[6, 9, 12]}\n",
    "             \n",
    "grid_search = GridSearchCV(estimator=XGBRegressor()\n",
    "                          ,param_grid=param_grid\n",
    "                          ,scoring='neg_root_mean_squared_error')\n",
    "\n",
    "grid_search.fit(training_data[input_features], training_data[target_field])\n",
    "\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Best cross-validation score: \", grid_search.best_score_ * -1)\n",
    "\n",
    "# Evaluate on test set using best estimator\n",
    "best_model = grid_search.best_estimator_\n",
    "test_score = best_model.score(test_data[input_features], test_data[target_field])\n",
    "print(\"Test set accuracy with best model: \", test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c8059c-990f-4998-a74d-590cb1176989",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
