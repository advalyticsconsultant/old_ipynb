{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9624eedc-b301-4625-8a2a-28daaba007cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8b7218-c4cc-4582-9892-65d09e4535d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"E:\\data\\superstore.xls\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6628bf1c-e890-4dd1-8228-b7bdb66dd60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(path)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ecff7c-55ce-4213-a79c-2f56ebe554ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['Profit'] <= 0, 'Profitable'] = False\n",
    "data.loc[data['Profit'] > 0, 'Profitable'] = True\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0490eec6-0150-46cc-87ee-195374ba6367",
   "metadata": {},
   "source": [
    "Let's create the following columns\n",
    "1. Order Year\n",
    "2. Order Day\n",
    "3. Order Month\n",
    "4. Profit Margin\n",
    "    - Profit / Sales\n",
    "6. Discount ($)\n",
    "    - Sales * Discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d3fa72-1d46-4668-b9b6-0f3f1f0998bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Order Year'] = data['Order Date'].dt.year\n",
    "data['Order Day'] = data['Order Date'].dt.strftime('%A')\n",
    "data['Order Month'] = data['Order Date'].dt.strftime('%B')\n",
    "data['Profit Margin'] = data['Profit'] / data['Sales']\n",
    "data['Discount ($)'] = data['Sales'] * data['Discount']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9c6b15-8c07-47ae-b370-2c4abc9516ac",
   "metadata": {},
   "source": [
    "Let's create another column (Discount Bucket), based on a criteria\n",
    "- for discount in the range 0% - 33% $\\implies$ Low\n",
    "- for discount in the range 33% - 66% $\\implies$ Medium\n",
    "- for discount i the range 66% - 100% $\\implies$ High"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8545bd-6500-499e-ab42-fa9447f856ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['Discount'] < 0.33, 'Discount Bucket'] = 'Low'\n",
    "data.loc[data['Discount'] >= 0.67, 'Discount Bucket'] = 'High'\n",
    "data.loc[(data['Discount'] >= 0.33) & (data['Discount'] < 0.67), 'Discount Bucket'] = 'Medium'\n",
    "data.loc[:, 'Discount Bucket']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a153668-247c-4853-9088-cb11282083f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['Discount Bucket'] == 'Medium', ['Discount', 'Discount Bucket']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0407f5-e700-459a-8f22-80bf4e25a0ed",
   "metadata": {},
   "source": [
    "### `dataframe_obj.drop(columns=[columns to remove], axis=1)`\n",
    "- removes columns from the dataframe by default (`axis=1`)\n",
    "- when `axis=0`, removes specified rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81b2f8d-1811-4811-8dc3-48e8fb7bdbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing columns from a dataframe\n",
    "data.drop(['Profitable'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7915f4-adb5-4d56-bdbe-493c9943ca15",
   "metadata": {},
   "source": [
    "### `dataframe_obj.unique()`\n",
    "- creates an array of unique values in the field specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f33b0b-113a-427b-9117-3a4c8b5a3a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Region'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c614da4-6e02-4311-881c-aa98e199517e",
   "metadata": {},
   "source": [
    "### `dataframe_obj.sort_values(by=[column_names], ascending=True, inplace=False, axis=0)`\n",
    "- used to sort values in ascending or descending order.\n",
    "- sort by ascending order by default; if you want to sort by descending order, specify `ascending=False`\n",
    "- we can also specify `inplace=True` to update the original dataframe\n",
    "- can sort columns also, by specifying `axis=1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e6a9c4-783b-4d8d-8818-a6b469f7ba22",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sort_values(by=['Order Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea38b444-9223-459e-9181-8495ba3b0bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting data by descending order of Profit\n",
    "data.sort_values(by=['Profit'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7891cc68-1fd8-49a8-9ba5-6e2597de50ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting multiple fields\n",
    "data.sort_values(by=['Order Date', 'Category', 'Profit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b6b01f-b94f-4f78-87d7-e4d8bdf85c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifying different sorting methods for different fields\n",
    "data.sort_values(by=['Order Date', 'Category', 'Profit'], ascending=[True, True, False])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9310ab-09fe-47cd-908a-0e187b628152",
   "metadata": {},
   "source": [
    "### `dataframe_obj.value_counts()`\n",
    "- used to create a simple frequency distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e339c2dc-e40a-4a46-aecc-2c53ff82c929",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.value_counts(subset='Customer ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f4aad3-1e7e-4d35-8fb2-252d8f119a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.value_counts(subset=['Category', 'Sub-Category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5250c925-3641-4f60-bf51-d2551eda86dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data.value_counts(subset=['Category', 'Sub-Category']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450a5bdd-40cd-4141-be28-e0f75514ab35",
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_count = data.value_counts(subset=['Category', 'Sub-Category'])\n",
    "transaction_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899cab50-ba8b-4076-be52-26d27b8a732f",
   "metadata": {},
   "source": [
    "### `dataframe_obj.reset_index(inplace=False)`\n",
    "- used to convert index of dataframe into a column and assign default index from 0 to n-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfb922a-33bb-485e-85d5-e11012d7bc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_count = transaction_count.reset_index()\n",
    "transaction_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f15862-31c3-478e-a9bd-04a6bc06e2b7",
   "metadata": {},
   "source": [
    "### `dataframe_obj.rename(columns={'old_name':'new_name'}, inplace=False)`\n",
    "- used to rename headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a849a7-92c6-41e2-91a7-3d940311aabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_count.rename(columns={'count':'Transactions'}, inplace=True)\n",
    "transaction_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba45ab82-924b-4aba-ad56-727efe7483f4",
   "metadata": {},
   "source": [
    "### Performing aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6994018d-3e44-4fed-8975-b7e909efb6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum of sales by category\n",
    "data.groupby('Category')['Sales'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fccd210-7770-4756-896c-ca0bfbc13cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum of sales by Category and Sub-Category\n",
    "data.groupby(['Category', 'Sub-Category'])['Sales'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8571530d-aba8-473e-b2a1-6116691dcd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum of Sales and Profit by Category and Sub-Category\n",
    "data.groupby(['Category', 'Sub-Category'])[['Sales', 'Profit']].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49bff03-afe6-4714-8320-27351b50d3ab",
   "metadata": {},
   "source": [
    "### Using `agg()` method to perform aggregations\n",
    "SYNTAX:\n",
    "```\n",
    "dataframe_obj.groupby([columns]).agg({field_1:[aggregation_methods]\n",
    "                                     ,field_2:[aggregation_methods]\n",
    "                                     ...\n",
    "                                     ,field_n:[aggregation_methods]]})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d62b336-25c0-42c0-9383-ed5bc0579a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the agg() method to perform multiple aggregations\n",
    "# sum of Sales and average of Sales by Category\n",
    "data.groupby(['Category']).agg({'Sales':['sum', 'mean']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbc0a32-e7ff-4ce5-98a9-9431d2a75c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum of Sales and Profit by Category and Sub-Category\n",
    "data.groupby(['Category', 'Sub-Category']).agg({'Sales':'sum'\n",
    "                                               ,'Profit':'sum'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d90c79-a34f-4edf-8da5-326313e5a663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum of Sales, sum of Profit and average Discount based on Category and Sub-Category\n",
    "data.groupby(['Category', 'Sub-Category']).agg({'Sales':'sum'\n",
    "                                               ,'Profit':'sum'\n",
    "                                               ,'Discount':'mean'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c507f98-7f1c-4aaa-8def-e170932df44c",
   "metadata": {},
   "source": [
    "### Using a standard syntax for `dataframe_obj.groupby().agg()`\n",
    "- there are multiple ways to use the `dataframe_obj.groupby().agg()` functionality\n",
    "- this may make the syntax confusing\n",
    "- a standard syntax to avoid errors can be\n",
    "  ```\n",
    "  dataframe_obj.groupby([columns_to_groupby]).agg({field_1:[aggregation_methods]\n",
    "                                                  ,field_2:[aggregation_methods]\n",
    "                                                  ...\n",
    "                                                  ,field_n:[aggregation_methods]})\n",
    "  ```\n",
    "\n",
    "- aggregation method keywords:\n",
    "  ```\n",
    "  count\n",
    "  mean\n",
    "  sum\n",
    "  median\n",
    "  mode\n",
    "  max\n",
    "  min\n",
    "  std\n",
    "  var\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219b825d-2d3e-40e8-b697-c825dcb1bf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count of transaction by each customer\n",
    "# previous syntax: data.value_counts(subset='Customer ID')\n",
    "# standard syntax:\n",
    "data.groupby(['Customer ID']).agg({'Customer ID':['count']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d528c05e-d731-4924-9424-296f3fc6c41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count of transactions in each sub-category\n",
    "# previous syntax: data.value_counts(subset=['Category', 'Sub-Category'])\n",
    "# standard syntax:\n",
    "data.groupby(['Category', 'Sub-Category']).agg({'Sub-Category':['count']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4f07e3-afc2-4fe5-a53e-66200b2c5708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum of sales by category\n",
    "# previous syntax: data.groupby('Category')['Sales'].sum()\n",
    "# standard syntax:\n",
    "data.groupby(['Category']).agg({'Sales':['sum']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db70ec58-874a-49e7-b174-9ebd013eb3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum of sales by Category and Sub-Category\n",
    "# previous syntax: data.groupby(['Category', 'Sub-Category'])['Sales'].sum()\n",
    "# standard syntax:\n",
    "data.groupby(['Category', 'Sub-Category']).agg({'Sales':['sum']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2ee486-6a23-4276-8f2d-38b85ae0eccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum of Sales and Profit by Category and Sub-Category\n",
    "# previous syntax: data.groupby(['Category', 'Sub-Category'])[['Sales', 'Profit']].sum()\n",
    "# standard syntax:\n",
    "data.groupby(['Category', 'Sub-Category']).agg({'Sales':['sum']\n",
    "                                               ,'Profit':['sum']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde328d9-ae14-4e8e-b94d-f0b201b2e630",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby(['Category', 'Sub-Category']).agg({'Sales':['sum']\n",
    "                                               ,'Profit':['sum']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a69a1f-b884-45de-81ae-ca728b21d1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_summary = data.groupby(['Category', 'Sub-Category']).agg({'Sales':['sum']\n",
    "                                                                  ,'Profit':['sum']})\n",
    "\n",
    "category_summary.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60688291-9335-4605-8821-815b7a75372d",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_summary.columns = category_summary.columns.map('_'.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b7b7f2-1cef-4b18-9fb8-0ea058fd2179",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ef8083-a44b-4aee-83d3-0a3b49771dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calulating average time it takes for a customer to order again\n",
    "# customer_data\n",
    "customer_data = data[['Customer ID', 'Order ID','Order Date']].drop_duplicates()\n",
    "customer_data.sort_values(['Customer ID', 'Order Date', 'Order ID'], inplace=True)\n",
    "customer_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53ab6f5-00c5-425a-af70-fe0bd36273fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_data['Next Order Date'] = customer_data['Order Date'].shift(-1)\n",
    "for customer in customer_data['Customer ID'].unique():\n",
    "    customer_data.loc[customer_data['Customer ID'] == customer, 'Next Order Date'] = customer_data.loc[customer_data['Customer ID'] == customer, 'Order Date'].shift(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08d9171-ec7f-4bc6-8141-db4c3209e8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_data['Duration'] = (customer_data['Next Order Date'] - customer_data['Order Date']).dt.days\n",
    "customer_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eac2770-e5c5-421e-8c06-86c1ee6a3bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_data.groupby(['Customer ID']).agg({'Order ID':'count'\n",
    "                                           ,'Duration':'mean'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de18d61-a49e-44dc-97ea-7ce31375a650",
   "metadata": {},
   "source": [
    "### General Data Clean-Up and Exploratory Data Analysis Guidelines\n",
    "Steps:\n",
    "1. ensure all fields are of the correct data type\n",
    "   - For example, data type of a date column should be date (or similar and not string)\n",
    "3. get an idea of null/non-null values\n",
    "    - can be done through `info()` method which tells us the number of non-null values in each column \n",
    "4. study each column of the data and get an understanding of values to identify invalid values\n",
    "    - this step involves standardising data so that we have proper values in the dataset\n",
    "    - in case of invalid values, we need to treat them as per business specification\n",
    "    - For example, in Superstore Orders table, for the `Category` field\n",
    "\n",
    "| Valid | Invalid |\n",
    "| --- | --- |\n",
    "| Technology | tech |\n",
    "| Furniture | Furn. |\n",
    "| Office Supplies | O. Supplies |\n",
    "| | Techno |\n",
    "| | Off. supp. |\n",
    "\n",
    "5. once data has been sanitized (all columns are of proper data type, all values are standardised), understand the metrics of the data\n",
    "    - for qualitative fields (dimensions): get the frequency distribution (number of times a value occurs)\n",
    "    - for quantitative fields (measures): get aggregate summary\n",
    "This step allows us to understand data points and also identify outliers\n",
    "    - outlier identification is important as it impacts our analysis\n",
    "    - we can use describe() method to get summary statistics for various columns in the DataFrame\n",
    "6. develop understanding of the relation between different pairs of columns and different combination of columns\n",
    "    - this allows us develop points of analysis\n",
    "    - for example, if sales in East are high, we should try to understand what is contributing to high sales in East region, is it due to:\n",
    "        - order frequency\n",
    "        - long-term customers\n",
    "        - more purchase of expensive products than inexpensive orders\n",
    "    - we should also analyse whether sales are uniformally high in all segments in the East region or are sales high only in Corporate segment\n",
    "7. based on the above analysis, we choose points which are of more importance than other points. These points are arranged and combined to create an end-to-end story about the data. As part of this compilation, we add visualizations in our reports to make it more accessible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4610e52f-a804-47a0-9616-0673fa09ba40",
   "metadata": {},
   "source": [
    "### Importing data by sheet name in pandas\n",
    "\n",
    "By default, when we don't specify a `sheet_name` in the `read_excel()` function, it imports data from the first sheet in the Excel workbook.\n",
    "\n",
    "We can import a specific sheet by specifying the name of the sheet in the `sheet_name` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e18b2a0-00ed-43da-b7d4-89c8b05bd6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = pd.read_excel(path, sheet_name='Returns')\n",
    "people = pd.read_excel(path, sheet_name='People')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02aa4e70-b18a-4504-9904-3ba48f83b8b8",
   "metadata": {},
   "source": [
    "### Listing sheet names in an Excel workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bb6579-4f02-422d-bcc2-08b32b0bbb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.ExcelFile(path).sheet_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e44eefd-08bd-4203-b57b-75cd36d549f1",
   "metadata": {},
   "source": [
    "### `dataframe_obj.merge(dataframe_2, on, left_on, right_on, how)`\n",
    "This method is used to combine two dataframes horizontally based on fields in the dataframe.\n",
    "- when we don't specify any fields it joins based on common field\n",
    "- we can combine them using `INNER JOIN` (default), `OUTER`, `CROSS`, `LEFT`, `RIGHT`, which we specify in the `how` parameter\n",
    "- merge also doesn't require us to rename columns in case there are common columns, it handles duplicate columns on its own\n",
    "\n",
    "NOTE: in case the key columns have different names, we can specify them:<br>\n",
    "`left_on` = name of key column in the left table<br>\n",
    "`right_on` = name of key column in the left table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042eb888-2e27-4b6a-b340-6bbc3257327a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(data, people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e7f32f-a8ac-4dec-9801-dc3639f647fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(data, returns, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6eddc5-5c1b-473a-bcc1-6ca92a5791d2",
   "metadata": {},
   "source": [
    "### `dataframe_obj.set_index()`\n",
    "The `dataframe_obj.set_index(['fields_to_set_as_index'])` is the opposite of `reset_index()` method. It takes in fields which we want to make into the index of the dataframe. This can be useful when we want to make the primary key of our table into an index, instead of keeping it as a column in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffdaf5d-1a25-4b7a-9426-187853dccaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = data\n",
    "orders.set_index(['Order ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac7460f-a18c-4b2b-84ee-4a1b1440b254",
   "metadata": {},
   "outputs": [],
   "source": [
    "returns.set_index(['Order ID'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b884827-4d1b-4091-811d-f2867b60efe1",
   "metadata": {},
   "source": [
    "### `dataframe_obj.join(dataframe_2, on, lsuffix, rsuffix, how)`\n",
    "- this method also combines data horizontally, but based on dataframe indices\n",
    "- we need to specify the suffix of at least one of the tables in case there are headers with the same name in both the tables\n",
    "- it performs `LEFT JOIN` by default, but can be changed using the `how` parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9517e462-8cbb-4a79-a611-5ae572e519dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.join(returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d23e7b8-72cb-4f52-9154-82a675973265",
   "metadata": {},
   "source": [
    "### Exporting dataframe to an Excel workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba675b24-7746-4517-81fe-a999981c7ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_file = pd.ExcelWriter(r'E:\\data\\orders.xlsx')\n",
    "\n",
    "for sub_category in data['Sub-Category'].unique():\n",
    "    data[data['Sub-Category'] == sub_category].to_excel(new_file, sheet_name=sub_category, index=False)\n",
    "\n",
    "new_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7dc94a-fb79-48a3-a7c1-759486753aca",
   "metadata": {},
   "source": [
    "### `pd.concat([dataframes], axis)`\n",
    "- this method is used to combine multiple dataframes into one\n",
    "- either vertically (default) or horizontally (axis=1)\n",
    "- usually used for `UNION`ing tables\n",
    "- the concat() method keeps duplicate rows, these can be removed using the drop_duplicates() method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c187a4be-6d58-45a3-8534-4b72cce88d67",
   "metadata": {},
   "source": [
    "### Importing multiple Excel sheets into one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a38f2bf-002d-41e7-83cc-18b3b1636119",
   "metadata": {},
   "outputs": [],
   "source": [
    "workbook_path = r'E:\\data\\orders.xlsx'\n",
    "\n",
    "sub_category_sheets = pd.ExcelFile(workbook_path).sheet_names\n",
    "sub_category_sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b16c65-b11a-41e0-8a9f-cc227becb4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_category_data = []\n",
    "for sheet in sub_category_sheets:\n",
    "    sub_category_data.append(pd.read_excel(workbook_path, sheet_name=sheet))\n",
    "\n",
    "pd.concat(sub_category_data)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a722b4fd-8d3e-4eb1-b456-1d5f06909b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_data = pd.concat(sub_category_data)\n",
    "orders_data.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307a5aa8-284e-4381-b9c5-a3f2d1eb6ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_data.drop(columns=['index'], inplace=True)\n",
    "orders_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2d3c6a-4469-44df-be5a-e2d47338a699",
   "metadata": {},
   "source": [
    "### Importing data from multiple CSV files in a folder into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539d46d7-905f-4315-b719-ba4fbc0ba7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "folder_path = r'E:\\data\\stocks_data'\n",
    "\n",
    "os.listdir(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa33fb1-dd10-40dc-bb85-b67ee0ef7888",
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_data = []\n",
    "for file in os.listdir(folder_path):\n",
    "    stocks_data.append(pd.read_csv(folder_path + '\\\\' + file))\n",
    "\n",
    "consolidated_data = pd.concat(stocks_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8bf7e7-4cf2-4707-9198-58a181d61c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "consolidated_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae1a672-ce42-4bf2-88d8-dad6a128c4c3",
   "metadata": {},
   "source": [
    "### Checking for duplicate rows\n",
    "\n",
    "When we are dealing with data dynamically, we should not hard-code any values. One method of fetching all the rows that are duplicate in the dataset can be as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d159e7ef-b4d6-4bb0-87d2-ce18bc5631f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1. place all headers in the table in the groupby method and get the count of any column in the table\n",
    "consolidated_data.groupby(list(consolidated_data.columns)).agg({consolidated_data.columns[0]:'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30c8024-4a19-4f64-8462-3f5843fa38b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing the above table into a new variable\n",
    "duplicate_check = consolidated_data.groupby(list(consolidated_data.columns)).agg({consolidated_data.columns[0]:'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a971c451-7ad7-499e-800e-ac3d9f6cecbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_check.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30849d01-00b8-461b-a6ed-6c7a1c28df50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2. storing the name of the column which contains count of duplicates into a variable\n",
    "count_header = duplicate_check.columns[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1132c6-3448-43f9-ad91-a2d721dfe6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3. fetching only those rows in the duplicate_check where the count is greater than 1\n",
    "duplicate_check[duplicate_check[count_header] > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696b9d5c-90e7-4867-953e-fc2d06a6db3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4. storing above data into a new variable after renaming the the field containing count to 'count'\n",
    "duplicate_rows = duplicate_check[duplicate_check[count_header] > 1].rename(columns={count_header:'count'})\n",
    "duplicate_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49165e48-1b62-4b83-9591-1b73fcf9b5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5. converting columns back into data fields from index\n",
    "duplicate_rows.reset_index(inplace=True)\n",
    "duplicate_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cec073-c46c-494d-986c-175d007df158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6. deleting the count column\n",
    "duplicate_rows.drop(columns=['count'], inplace=True)\n",
    "duplicate_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce1df76-b580-4981-bce2-ce9aed4a1c71",
   "metadata": {},
   "source": [
    "### Downloading a file from a URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d2c114-00ee-44f1-8491-8ee1da93eb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "\n",
    "specifying URL of the file\n",
    "url = r'https://raw.githubusercontent.com/puneettrainer/datasets/main/insurance_fraud.csv'\n",
    "\n",
    "extracting file name from the URL\n",
    "filename = url.split('/')[-1]\n",
    "\n",
    "downloading the file and saving it under the file name extracted\n",
    "urlretrieve(url, filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
